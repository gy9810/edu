# 说明文档



## 任务完成情况

- Task 1 (required)：已完成，没有用到第三方库
- The BONUS of Task 1：已完成

&emsp;&emsp;**对BONUS问题的回答：**对于规模庞大的语料库，如果依然考虑使用传统的离散式文本表示方法，例如one-hot编码以及tf-idf编码

等，词表的维度将随着语料库的增长而迅速膨胀，尤其是n-gram词序列会呈指数爆炸增长。

&emsp;&emsp;因此可以考虑和深度学习结合，用分布式的方法来表示文本：将所有词语投影到 $K$ 维的向量空间，每个词语都可以用一个 $K$ 维向量

表示。分布式表示最大的优点是具备非常强的特征表达能力，比如 $n$ 维向量每维 $k$ 个值，可以表征 $k^n$ 个概念。这样的表达能力，意味着

在对文本的特征提取过程中可以大大缩减数据量。

&emsp;&emsp;这里我以文本分类问题为例，可以设计如下图的一个结构：

![image](https://github.com/gy9810/edu/blob/master/image-20210612000654272.png)

&emsp;&emsp;基本思路就是，词（或者字）经过嵌入（embedding）层之后，利用 CNN/RNN 等结构，提取局部信息、全局信息或上下文信息，

最后利用分类器进行分类，分类器可以由几层全连接层组成。至于中间可能涉及的 TextCNN，TextRNN 等文本分类模型，我就不详细介

绍了。



## Task 1项目设计及源码简要说明

&emsp;&emsp;Task 1的要求就是对给定的输入数据文件`corpus.json`，针对其中文本内容，分别计算其uni-gram，bi-gram，tri-gram的TF-IDF值。

&emsp;&emsp;我的设计思路就是先将每个document中的文本内容，即`pageTitle`，`subTitleS`和`body`提取出来，然后拼接在一起当做一个完整

的文本，进行文本清洗和拆分工作。文本清洗的内容包括：转换为小写、英文缩写的还原、去除杂乱符号、用空格替换换行符等，然后以

空格为分隔符，返回文本列表。

&emsp;&emsp;遍历该列表，计算 n-gram，这部分的核心代码比较简单，如下所示（n 分别取1、2、3，表示 uni-gram，bi-gram，tri-gram）：

```pyth
for i in range(len(input) - n):
	ngramTemp = " ".join(input[i:i + n])
```

&emsp;&emsp;同时统计各n-gram的出现次数，将这些结果以字典形式保存下来( key为n-gram，value为出现次数 )，然后将字典累加可得到总出现

次数，同时也可以统计出该n-gram在多少个document中出现过。有了这些数据即可分别计算出 TF 值和 IDF 值，也就得到了TF-IDF值。

最后就是json格式数据的输出，这些我就不细说了。



## 代码运行方式

我的程序包含一个data文件夹，还有两个.py文件：`get_TFIDF.py`和`n_gram.py`

data文件夹存放了输入数据文件`corpus.json`，以及任务要求的两个输出文件`global_vocab.json`和`corpus_with_tokens.json`

运行`get_TFIDF.py`即可在data文件夹中生成`global_vocab.json`和`corpus_with_tokens.json`



## 对输出的解释

&emsp;&emsp;`global_vocab.json`中有输入文本所有的uni-gram，bi-gram，tri-gram，以及其对应的doc_frequency，frequency，idf和tfdif。

前面那些指标主要还是为了计算TF-IDF值，这是用于提取文档关键词的一个重要指标，这个值越大，这个n-gram就越能代表这篇文档，

即它越可能被选为文档的关键词。

&emsp;&emsp;`corpus_with_tokens.json`就是在原本的输入文件`corpus.json`中，对每个document增加一个`tokens`的字段值，`tokens`里包含

了这个document中所有可能的uni-gram，bi-gram，tri-gram，以及其对应的frequency（这里的frequency指的是在该document中

n-gram的出现次数，与`global_vocab.json`中不同）和tfidf，简单来说也是体现该n-gram对该document的重要程度，frequency和

tfidf越大，该n-gram越重要。



## 未来可能的改进工作

&emsp;&emsp;因为没有使用nltk，sklearn之类的第三方库，所以有些方面的处理比较粗糙。尤其是文本拆分和文本清洗的部分，比如英文缩写的还

原，我只是处理了一些常见的搭配。停用词的去除和http网址的清理也暂未实现，所以会有一些无用的字符串出现，计算出的TF-IDF值也

不是那么准确，但还是有一定的参考价值。如果未来继续改进这个任务，我会考虑增加停用词的去除，还有对于url格式的处理，以使得

程序变得更加准确和可靠。



## 完成任务时的个人时间分配

&emsp;&emsp;我于6.9下午收到题目，当晚仔细阅读了一下题目的意思和具体要求，同时查阅了一些相关资料，然后开始在脑中对Task 1构建思

路。6.10下午继续在网上查阅题目中涉及的概念和算法的相关资料，然后开始着手编程，先是读取json文件，然后构建n-gram。在我想

要继续计算TF-IDF值的时候，发现自己对于这个概念的理解可能存在误区，于是当晚发送邮件给Cicada Speech的面试官们进行确认，在

第二天的早上，也就是6.11上午得到了技术总监的回复，解答了我的疑惑。然后我就继续编写了计算TF-IDF值的代码，6.11下午我完成了

Task 1的收尾工作，同时对程序进行一些测试，最后调整了程序的输出格式。

&emsp;&emsp;6.11下午剩余的时间和当天晚上，我都在思考The BONUS of Task 1的问题，在网上查询了nlp以及大数据处理相关的资料，同时与

自己做过的深度神经网络相关工作进行了结合思考，最后有了一些粗略的想法。6.12白天在监考英语四六级考试，所以在6.12晚上进行了

说明文档的编写。

&emsp;&emsp;另外，关于Task 2，我查询资料了解到，除了TF-IDF，还有TextRank关键词提取算法，LDA主题模型关键词提取算法，Word2Vec词

聚类的关键词提取算法等等。但因为我6.16还有考试，需要腾出一些时间复习课程，所以暂时没有太多精力去对这些算法做一个实现和对

比，Task 2的完成暂时无法实现，实在是很抱歉。
